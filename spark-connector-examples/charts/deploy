apiVersion: "spark.nautilus.dellemc.com/v1beta1"
kind: SparkApplication
metadata:
  name: spark-pi
spec:
  # Language type of application, Python, Scala, Java  (optional: Defaults to Java if not specified)
  type: Python
  
  # Can be Running, Finished or Stopped (required)
  state: Running

  # Signifies what the `mainApplicationFile` value represents.  Valid Values: "Maven", "File", "Text".  Default: "Text"
  mainApplicationFileType: File
 
  # Main application file that will be passed to Spark Submit (interpreted according to `mainApplicationFileType`)
  mainApplicationFile: "stream_generated_data_to_pravega.py"
   
  # Extra Maven dependencies to resolve from Maven Central (optional: Java/Scala ONLY)
  # packages:                      
  #   - org.spark:examples:2.11-2.4.5
  #   - com.dellemc:pravega:test:0.1
 
  # List of transient dependencies to exclude when resolving Extra Maven dependencies (optional: Java/Scala ONLY)
  # excludePackages:
  #   - apache:log4j:2.11-2.4.5
 
  # URLs of any python code (.py, .zip or .egg) files that should be retrieved and add to the Python Driver/Executor runtime (optional: Python ONLY)
  # pyFiles:
  #   - "{file: stream_generated_data_to_pravega.py}"
 
  # Any Data files that should be retrieved and placed in the working directory of the executors (optional)
  # files:                       
  #   - http://sdp.dellemc.myjobs/lookup-table.csv
  
  # Key/Value parameters passed as arguments to application (optional)
  parameters:                    
    - name: timeout
      value: "100000"
  
    - name: password
      type: Secret
      value: DatabasePassword
  
  # Single value arguments passed to application (optional)
  arguments:
    - "--recreate"
 
  # Directory in project storage which will be handed to Application as CHECKPOINT_DIR environment variable for Application checkpoint files (optional, SDP will use Analytic Project storage)
  checkpointPath: /spark-pi-checkpoint-d5722b45-8773-41db-93a4-bab2324d74d0
 
  # Number of seconds SDP will wait after signalling shutdown to an application before forcabily killing the Driver POD (optional, default: 180 seconds)
  gracefulShutdownTimeout: 180
 
  # When to redeploy the application (optional, default: OnFailure, failureRetries: 3, failureRetryInterval: 10, submissionRetries: 0)
  retryPolicy:
    # Valid values: Never, Always, OnFailure
    type: Always
 
    # Number of retries before Application is marked as failed
    failureRetries: 5
 
    # Number of seconds between each retry during application failures
    failureRetryInterval: 60
 
    # Number of submission failures before Application is marked as Failed
    submissionFailureRetries: 5
 
    # Number of seconds between each submission retry on failure
    submissionFailureRetryInterval: 60
 
  # Defines shape of cluster to execute application
  clusterTemplate:
    # Reference to a RuntimeImage containing the Spark Runtime to use for the cluster
    runtime: spark-2.4.6
          
    # Volumes to be mounted into Driver and Executor Pods. VolumeSource is a Kubernetes standard volumeSource (optional)
    volumes:
        - name: myVolume  
          volumeSource:
            emptyDir: {}
 
    # Driver Resource settings
    driver:
      cores: 1
      memory: "512m"
 
      # Volumes to be mounted in driver (name must match volume name, optional)
      volumeMounts:
        - name: myVolume
          mountPath: /mnt/test/myvolume
 
      # Kubernetes resources that should be applied to the POD (optional)
      resources:
          requests:
            cpu: 1
          limits:
            cpu: 1
  
    # Executor Resource Settings
    executor:
      replicas: 3
      cores: 1
      memory: "512m"
  
      # Volumes to be mounted in executors (name must match volume name, optional)
      volumeMounts:
        - name: myVolume
          mountPath: /mnt/test/myvolume
 
      # Storage to use for Spark scratch space (output files and RDDs)
      # Ephemeral Storage will be used if this is missing
      localStorage:
        replicas: 1     
        volumeClaimTemplate:
          accessModes: [ "ReadWriteOnce" ]
          storageClassName: "standard"
          resources:
            requests:
              storage: 20Gi
 
      # Kubernetes resources that should be applied to each executor POD
      # resources:
      #   requests:
      #       nvidia.com/gpu: 2
      #     limits:
      #       nvidia.com/gpu: 2
 
    # Extra key value/pairs to apply to configuration (optional)
    configuration:
      spark.reducer.maxSizeInFlight: 48m
 
    # Custom Log4J Logging Levels (optional)
    logging:
        org.myapp: DEBUG